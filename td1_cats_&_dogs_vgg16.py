# -*- coding: utf-8 -*-
"""TD1 - Cats & dogs VGG16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xZq2K4ts49WiDCVB6EYFryfNqQSIwu4e
"""

from google.colab import drive
import zipfile
import os
drive.mount("/content/drive", force_remount=True)

# !wget --no-check-certificate \
#     "https://www.kaggle.com/datasets/phucthaiv02/butterfly-image-classification/download?datasetVersionNumber=1" \
#     -O "/tmp/butterfly.zip"

# local_zip = '/tmp/butterfly.zip'
# zip_ref = zipfile.ZipFile(local_zip, 'r')
# zip_ref.extractall('/tmp')
# zip_ref.close()

print(f"Total number of Cats is {len(os.listdir('/tmp/PetImages/Cat/'))}")
# Total number of Dogs.
print(f"Total number of Dogs is {len(os.listdir('/tmp/PetImages/Dog/'))}")

try:
  os.mkdir("/tmp/cats-vs-dogs")
  os.mkdir("/tmp/cats-vs-dogs/training")
  os.mkdir("/tmp/cats-vs-dogs/validation")
  os.mkdir("/tmp/cats-vs-dogs/training/Cats")
  os.mkdir("/tmp/cats-vs-dogs/training/Dogs")
  os.mkdir("/tmp/cats-vs-dogs/validation/Cats")
  os.mkdir("/tmp/cats-vs-dogs/validation/Dogs")
except OSError:
  pass

from shutil import  copyfile
import random
def split_data(SOURCE, TRAINING, VALIDATION, SPLIT_SIZE):
  files = []
  for filename in os.listdir(SOURCE):
    file = SOURCE + filename
    if os.path.getsize(file) > 0:
      files.append(filename)
    else:
      print(filename, "is zero length, so ignoring!")

  training_length = int(len(files) * SPLIT_SIZE)
  validation_length = int(len(files) - training_length)
  shuffled_set = random.sample(files, len(files))
  training_set = shuffled_set[0:training_length]
  validation_set = shuffled_set[0:validation_length]

  for filename in training_set:
    this_file = SOURCE + filename
    destination = TRAINING + filename
    copyfile(this_file, destination)

  for filename in validation_set:
    this_file = SOURCE + filename
    destination = VALIDATION + filename
    copyfile(this_file, destination)


CAT_SOURCE_DIR = "/tmp/PetImages/Cat/"
TRAINING_CAT_DIR = "/tmp/cats-vs-dogs/training/Cats/"
VALIDATION_CAT_DIR = "/tmp/cats-vs-dogs/validation/Cats/"

DOG_SOURCE_DIR = "/tmp/PetImages/Dog/"
TRAINING_DOG_DIR = "/tmp/cats-vs-dogs/training/Dogs/"
VALIDATION_DOG_DIR = "/tmp/cats-vs-dogs/validation/Dogs/"

SPLIT_SIZE = 0.9

split_data(CAT_SOURCE_DIR, TRAINING_CAT_DIR, VALIDATION_CAT_DIR, SPLIT_SIZE)
split_data(DOG_SOURCE_DIR, TRAINING_DOG_DIR, VALIDATION_DOG_DIR, SPLIT_SIZE)

print(f"Total number of training Cats is {len(os.listdir('/tmp/cats-vs-dogs/training/Cats/'))}")
print(f"Total number of training Dogs is {len(os.listdir('/tmp/cats-vs-dogs/training/Dogs/'))}")

# Total number of images in Validation
print(f"Total number of validation Cats is {len(os.listdir('/tmp/cats-vs-dogs/validation/Cats/'))}")
print(f"Total number of validation Dogs is {len(os.listdir('/tmp/cats-vs-dogs/validation/Dogs/'))}")

train_data_dir = os.path.join("/tmp/cats-vs-dogs/training/")


# Directory with validation cats images
validation_data_dir = os.path.join("/tmp/cats-vs-dogs/validation/")

import tensorflow as tf
base_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
for layers in base_model.layers:
   layers.trainable = False
x = tf.keras.layers.Flatten(name='flatten')(base_model.output)
x = tf.keras.layers.Dense(4096, activation='relu')(x)
x = tf.keras.layers.Dense(4096, activation='relu')(x)
x = tf.keras.layers.Dense(2,  activation='softmax')(x) #change number of class into value 2

model = tf.keras.models.Model(inputs=base_model.input,outputs=x, name='VGG16')
opti_fun = tf.keras.optimizers.Adam(lr =1e-4)
loss_fun = 'categorical_crossentropy'
model.compile(optimizer=opti_fun, loss=loss_fun, metrics=['accuracy'])

model.summary()

train_data_dir='/content/drive/MyDrive/Computer vision/dogvscat/train'
validation_data_dir='/content/drive/MyDrive/Computer vision/dogvscat/test'

train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)
val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
      train_data_dir,
      target_size=(224, 244),
      batch_size= 32,
      shuffle=True,
      class_mode='categorical')

validation_generator = val_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(224, 224),
    batch_size=255,
    class_mode='categorical')

# save the model
model_save_path='/content/drive/MyDrive/Colab Notebooks/Computer vision/dogvcat'
model_checkpoint = tf.keras.callbacks.ModelCheckpoint(model_save_path+model.name+'.hdf5',monitor='val_loss',  verbose=1,save_best_only=True)
history = model.fit_generator(train_generator,
                                  epochs=2,validation_data=validation_generator,
                                  callbacks=[model_checkpoint])

import cv2
import matplotlib.pyplot as plt
import numpy as np
# load the model
base_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Computer vision/dogvcatVGG16.hdf5')

dictionary=['dog','cat']

from PIL import Image
img=cv2.imread('/content/374872660_24418574611066748_7147810266860830260_n.jpeg')

img_RGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_RGB = img_RGB / 255.0
img_RGB = cv2.resize(img_RGB, (224, 224))
img_batch = np.reshape(img_RGB, (1, 224, 224, 3))
pred = base_model.predict(img_batch)
index = np.argmax(pred[0])


image_test=Image.open('/content/374872660_24418574611066748_7147810266860830260_n.jpeg')
newsize = (224, 224)
image_test = image_test.resize(newsize)
fig = plt.figure()
plt.imshow(image_test)

plt.title("This is a "+dictionary[index],fontsize=20)
plt.imshow(img)
plt.show()